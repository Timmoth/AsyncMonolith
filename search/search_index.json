{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AsyncMonolith AsyncMonolith is a lightweight library that facilitates simple, durable and asynchronous messaging in dotnet apps. Overview: Speed up your API by offloading tasks to a background worker. Distribute workload amongst multiple app instances. Execute tasks at specific times or after a delay. Execute tasks at regular intervals. Simplify complex processes by building out an event-driven architecture. Decouple your services by utilizing the Mediator pattern. Improve your application's resiliency by utilizing the Transactional Outbox pattern. Improve your application's resiliency by utilizing automatic retries. Keep your infrastructure simple without using a message broker. Simplify testability. [!NOTE] Despite its name, AsyncMonolith can be used within a microservices architecture. The only requirement is that the producers and consumers share the same database i.e messaging within a single project. However, it is not suitable for passing messages between different projects in a microservices architecture, as microservices should not share the same database. Note Despite its name, AsyncMonolith can be used within a microservices architecture. The only requirement is that the producers and consumers share the same database i.e messaging within a single project. However, it is not suitable for passing messages between different projects in a microservices architecture, as microservices should not share the same database. Support \ud83d\udedf Need help? Ping me on linkedin and I'd be more then happy to jump on a call to debug, help configure or answer any questions.","title":"Overview \u2705"},{"location":"#asyncmonolith","text":"AsyncMonolith is a lightweight library that facilitates simple, durable and asynchronous messaging in dotnet apps.","title":"AsyncMonolith"},{"location":"#overview","text":"Speed up your API by offloading tasks to a background worker. Distribute workload amongst multiple app instances. Execute tasks at specific times or after a delay. Execute tasks at regular intervals. Simplify complex processes by building out an event-driven architecture. Decouple your services by utilizing the Mediator pattern. Improve your application's resiliency by utilizing the Transactional Outbox pattern. Improve your application's resiliency by utilizing automatic retries. Keep your infrastructure simple without using a message broker. Simplify testability. [!NOTE] Despite its name, AsyncMonolith can be used within a microservices architecture. The only requirement is that the producers and consumers share the same database i.e messaging within a single project. However, it is not suitable for passing messages between different projects in a microservices architecture, as microservices should not share the same database. Note Despite its name, AsyncMonolith can be used within a microservices architecture. The only requirement is that the producers and consumers share the same database i.e messaging within a single project. However, it is not suitable for passing messages between different projects in a microservices architecture, as microservices should not share the same database.","title":"Overview:"},{"location":"#support","text":"Need help? Ping me on linkedin and I'd be more then happy to jump on a call to debug, help configure or answer any questions.","title":"Support \ud83d\udedf"},{"location":"contributing/","text":"Contributing \ud83d\ude4f Contributions are welcome! Here\u2019s how you can get involved: Fork the repository : Click the \"Fork\" button at the top right of this page. Clone your fork : git clone https://github.com/Timmoth/AsyncMonolith.git Create a branch : Make your changes in a new branch. git checkout -b my-feature-branch Commit your changes : git commit -m 'Add some feature' Push to the branch : git push origin my-feature-branch Open a pull request : Describe your changes and submit your PR.","title":"Contributing \ud83d\ude4f"},{"location":"contributing/#contributing","text":"Contributions are welcome! Here\u2019s how you can get involved: Fork the repository : Click the \"Fork\" button at the top right of this page. Clone your fork : git clone https://github.com/Timmoth/AsyncMonolith.git Create a branch : Make your changes in a new branch. git checkout -b my-feature-branch Commit your changes : git commit -m 'Add some feature' Push to the branch : git push origin my-feature-branch Open a pull request : Describe your changes and submit your PR.","title":"Contributing \ud83d\ude4f"},{"location":"demo/","text":"Hit https://localhost:60046/api/spam?count=1000 to see how performant AsyncMonolith is on your system. With 10 message batches and single processor instance I usually process (trivial) messages at <10ms each. The demo is setup to run against a PostgreSql database, make sure you've got docker installed","title":"Demo App \u2728"},{"location":"internals/","text":"ProducerService Resolves consumers for a given payload and writes messages to the consumer_messages table for processing. ScheduleService Writes scheduled messages to the scheduled_messages table. DbSet: ConsumerMessage Stores all messages awaiting processing by the ConsumerMessageProcessor . DbSet: ScheduledMessage Stores all scheduled messages awaiting processing by the ScheduledMessageProcessor . DbSet: PoisonedMessage Stores consumer messages that have reached AsyncMonolith.MaxAttempts , poisoned messages will then need to be manually moved back to the consumer_messages table or deleted. ConsumerMessageProcessor A background service that periodically fetches available messages from the 'consumer_messages' table. Once a message is found, it's row-level locked to prevent other processes from fetching it. The corresponding consumer attempts to process the message. If successful, the message is removed from the consumer_messages table; otherwise, the processor increments the messages attempts by one and delays processing for a defined number of seconds ( AsyncMonolithSettings.AttemptDelay ). If the number of attempts reaches the limit defined by AsyncMonolith.MaxAttempts , the message is moved to the poisoned_messages table. ScheduledMessageProcessor A background service that fetches available messages from the scheduled_messages table. Once found, each consumer set up to handle the payload is resolved, and a message is written to the consumer_messages table for each of them. ConsumerRegistry Used to resolve all the consumers able to process a given payload, and resolve instances of the consumers when processing a message. The registry is populated on startup by calling builder.Services.AddAsyncMonolith<ApplicationDbContext>(Assembly.GetExecutingAssembly()); which uses reflection to find all consumer & payload types. Notes \ud83d\udccb The background services wait for AsyncMonolithSettings.ProcessorMaxDelay seconds before fetching another batch of messages. If a full batch is fetched, the delay is reduced to AsyncMonolithSettings.ProcessorMinDelay seconds between cycles. Configuring concurrent consumer / scheduled message processors will throw a startup exception when using AsyncMonolith.Ef (due to no built in support for row level locking)","title":"Internals \ud83e\udde0"},{"location":"internals/#producerservice","text":"Resolves consumers for a given payload and writes messages to the consumer_messages table for processing.","title":"ProducerService"},{"location":"internals/#scheduleservice","text":"Writes scheduled messages to the scheduled_messages table.","title":"ScheduleService"},{"location":"internals/#dbset-consumermessage","text":"Stores all messages awaiting processing by the ConsumerMessageProcessor .","title":"DbSet: ConsumerMessage"},{"location":"internals/#dbset-scheduledmessage","text":"Stores all scheduled messages awaiting processing by the ScheduledMessageProcessor .","title":"DbSet: ScheduledMessage"},{"location":"internals/#dbset-poisonedmessage","text":"Stores consumer messages that have reached AsyncMonolith.MaxAttempts , poisoned messages will then need to be manually moved back to the consumer_messages table or deleted.","title":"DbSet: PoisonedMessage"},{"location":"internals/#consumermessageprocessor","text":"A background service that periodically fetches available messages from the 'consumer_messages' table. Once a message is found, it's row-level locked to prevent other processes from fetching it. The corresponding consumer attempts to process the message. If successful, the message is removed from the consumer_messages table; otherwise, the processor increments the messages attempts by one and delays processing for a defined number of seconds ( AsyncMonolithSettings.AttemptDelay ). If the number of attempts reaches the limit defined by AsyncMonolith.MaxAttempts , the message is moved to the poisoned_messages table.","title":"ConsumerMessageProcessor"},{"location":"internals/#scheduledmessageprocessor","text":"A background service that fetches available messages from the scheduled_messages table. Once found, each consumer set up to handle the payload is resolved, and a message is written to the consumer_messages table for each of them.","title":"ScheduledMessageProcessor"},{"location":"internals/#consumerregistry","text":"Used to resolve all the consumers able to process a given payload, and resolve instances of the consumers when processing a message. The registry is populated on startup by calling builder.Services.AddAsyncMonolith<ApplicationDbContext>(Assembly.GetExecutingAssembly()); which uses reflection to find all consumer & payload types.","title":"ConsumerRegistry"},{"location":"internals/#notes","text":"The background services wait for AsyncMonolithSettings.ProcessorMaxDelay seconds before fetching another batch of messages. If a full batch is fetched, the delay is reduced to AsyncMonolithSettings.ProcessorMinDelay seconds between cycles. Configuring concurrent consumer / scheduled message processors will throw a startup exception when using AsyncMonolith.Ef (due to no built in support for row level locking)","title":"Notes \ud83d\udccb"},{"location":"quickstart/","text":"For a more detailed example look at the Demo project Install the nuget package to support your database // Pick one dotnet add package AsyncMonolith . Ef dotnet add package AsyncMonolith . MySql dotnet add package AsyncMonolith . MsSql dotnet add package AsyncMonolith . PostgreSql dotnet add package AsyncMonolith . MariaDb Setup your DbContext public class ApplicationDbContext : DbContext { public ApplicationDbContext ( DbContextOptions < ApplicationDbContext > options ) : base ( options ) { } // Add Db Sets public DbSet < ConsumerMessage > ConsumerMessages { get ; set ; } = default ! ; public DbSet < PoisonedMessage > PoisonedMessages { get ; set ; } = default ! ; public DbSet < ScheduledMessage > ScheduledMessages { get ; set ; } = default ! ; // Configure the ModelBuilder protected override void OnModelCreating ( ModelBuilder modelBuilder ) { modelBuilder . ConfigureAsyncMonolith (); base . OnModelCreating ( modelBuilder ); } } If you're not using ef migrations check out the sql to configure your database here Register your dependencies and configure settings // Register required services builder . Services . AddLogging (); builder . Services . AddSingleton ( TimeProvider . System ); // Register AsyncMonolith using either: // services.AddEfAsyncMonolith // services.AddMsSqlAsyncMonolith // services.AddMySqlAsyncMonolith // services.AddPostgreSqlAsyncMonolith // services.AddMariaDbAsyncMonolith builder . Services . AddPostgreSqlAsyncMonolith < ApplicationDbContext > ( settings => { settings . RegisterTypesFromAssembly ( Assembly . GetExecutingAssembly ()); settings . AttemptDelay = 10 , // Seconds before a failed message is retried settings . MaxAttempts = 5 , // Number of times a failed message is retried settings . ProcessorMinDelay = 10 , // Minimum millisecond delay before the next batch is processed settings . ProcessorMaxDelay = 1000 , // Maximum millisecond delay before the next batch is processed settings . ProcessorBatchSize = 5 , // The number of messages to process in a single batch settings . ConsumerMessageProcessorCount = 2 , // The number of concurrent consumer message processors to run in each app instance settings . ScheduledMessageProcessorCount = 1 , // The number of concurrent scheduled message processors to run in each app instance settings . DefaultConsumerTimeout = 10 // The default number of seconds before a consumer will timeout }); The following methods are available on the AsyncMonolithSettings passed into the settings lambda: RegisterTypesFromAssemblyContaining<T>() RegisterTypesFromAssemblyContaining(Type type) RegisterTypesFromAssembly(Assembly assembly) RegisterTypesFromAssemblies(params Assembly[] assemblies) Create messages and consumers // Define Consumer Payload public class ValueSubmitted : IConsumerPayload { [JsonPropertyName(\"value\")] public required double Value { get ; set ; } } // Define Consumer [ConsumerTimeout(5)] // Consumer timeouts after 5 seconds [ConsumerAttempts(1)] // Consumer messages moved to poisoned table after 1 failed attempt public class ValueSubmittedConsumer : BaseConsumer < ValueSubmitted > { private readonly ApplicationDbContext _dbContext ; private readonly IProducerService _producerService ; public ValueSubmittedConsumer ( ApplicationDbContext dbContext , IProducerService producerService ) { _dbContext = dbContext ; _producerService = producerService ; } public override Task Consume ( ValueSubmitted message , CancellationToken cancellationToken ) { ... await _dbContext . SaveChangesAsync ( cancellationToken ); } } Produce / schedule messages // Inject services private readonly IProducerService _producerService ; private readonly IScheduleService _scheduleService ; // Produce a message to be processed immediately await _producerService . Produce ( new ValueSubmitted () { Value = newValue }); // Produce a message to be processed in 10 seconds await _producerService . Produce ( new ValueSubmitted () { Value = newValue }, 10 ); // Produce a message to be processed in 10 seconds, but only once for a given userId await _producerService . Produce ( new ValueSubmitted () { Value = newValue }, 10 , $\"user:{userId}\" ); // Publish a message every Monday at 12pm (UTC) with a tag that can be used to modify / delete related scheduled messages. _scheduleService . Schedule ( new ValueSubmitted { Value = newValue }, \"0 0 12 * * MON\" , \"UTC\" , \"id:{id}\" ); // Save changes await _dbContext . SaveChangesAsync ( cancellationToken );","title":"Quick start \u25b6\ufe0f"},{"location":"releases/","text":"Make sure to check this table before updating the nuget package in your solution, you may be required to add an dotnet ef migration . Version Description Migration Required 8.0.6 Added ConsumerAttempts override attribute No 8.0.5 Bundle debug symbols with nuget package No 8.0.4 Bundle XML docs with nuget package No 8.0.3 Optimised Sql No 8.0.2 Added distributed trace_id and span_id No 8.0.1 Added OpenTelemetry support Yes 8.0.0 Use Producer & Schedule service interfaces No 1.0.9 Initial Yes If you're not using ef migrations check out the sql to configure your database here","title":"Releases \ud83d\udcd2"},{"location":"support/","text":"Need help? Ping me on linkedin and I'd be more then happy to jump on a call to debug, help configure or answer any questions.","title":"Support \ud83d\udedf"},{"location":"tests/","text":"AsyncMonolith.Tests Some of the test rely on TestContainers to run against real databases, make sure you've got docker installed AsyncMonolith.TestHelpers Install the TestHelpers package to help with unit / integration tests. FakeIdGenerator Generates sequential fake id's of the format $\"fake-id-{invocationCount}\" ConsumerMessageTestHelpers Static methods for asserting messages have been inserted into the consumer_messages table. FakeProducerService Provides a fake IProducerService implementation, which is useful for asserting messages have been published without using a database. FakeScheduleService Provides a fake IScheduleService implementation, which is useful for asserting messages have been scheduled without using a database. SetupTestHelpers Includes two methods to configure your IServiceCollection without including the background services for processing messages. AddFakeAsyncMonolithBaseServices configures fake services which don't depend on a database AddRealAsyncMonolithBaseServices configures real services without registering the background processors TestConsumerMessageProcessor Static methods for invoking and testing your consumers. ConsumerTestBase The ConsumerTestBase offers a way to write simple tests for your consumers public class CancelShipmentTests : ConsumerTestBase { public CancelShipmentTests ( ITestOutputHelper testOutputHelper ) : base ( testOutputHelper ) { } private readonly string _inMemoryDatabaseName = Guid . NewGuid (). ToString (); protected override Task Setup ( IServiceCollection services ) { services . AddDbContext < ApplicationDbContext > (( sp , options ) => { options . UseInMemoryDatabase ( _inMemoryDatabaseName ); } ); services . AddRealAsyncMonolithBaseServices < ApplicationDbContext > ( settings => settings . RegisterTypesFromAssemblyContaining < Program > ()); return Task . CompletedTask ; } [Fact] public async Task OrderCancelled_Sets_Shipment_Status_To_Cancelled () { // Given var model = new Shipment { Id = \"test-shipment-id\" , OrderId = \"test-order-id\" , Status = \"pending\" , }; using ( var scope = Services . CreateScope ()) { var dbContext = scope . ServiceProvider . GetRequiredService < ApplicationDbContext > (); dbContext . Shipments . Add ( model ); await dbContext . SaveChangesAsync (); } // When await Process < CancelShipmentConsumer , OrderCancelled > ( new OrderCancelled () { OrderId = model . OrderId }); // Then using ( var scope = Services . CreateScope ()) { var dbContext = scope . ServiceProvider . GetRequiredService < ApplicationDbContext > (); var shipment = await dbContext . Shipments . FirstOrDefaultAsync ( c => c . Id == model . Id ); shipment . Status . Should (). Be ( \"cancelled\" ); await dbContext . SaveChangesAsync (); } } }","title":"Tests \ud83d\udc1e"},{"location":"tests/#asyncmonolithtests","text":"Some of the test rely on TestContainers to run against real databases, make sure you've got docker installed","title":"AsyncMonolith.Tests"},{"location":"tests/#asyncmonolithtesthelpers","text":"Install the TestHelpers package to help with unit / integration tests.","title":"AsyncMonolith.TestHelpers"},{"location":"tests/#fakeidgenerator","text":"Generates sequential fake id's of the format $\"fake-id-{invocationCount}\"","title":"FakeIdGenerator"},{"location":"tests/#consumermessagetesthelpers","text":"Static methods for asserting messages have been inserted into the consumer_messages table.","title":"ConsumerMessageTestHelpers"},{"location":"tests/#fakeproducerservice","text":"Provides a fake IProducerService implementation, which is useful for asserting messages have been published without using a database.","title":"FakeProducerService"},{"location":"tests/#fakescheduleservice","text":"Provides a fake IScheduleService implementation, which is useful for asserting messages have been scheduled without using a database.","title":"FakeScheduleService"},{"location":"tests/#setuptesthelpers","text":"Includes two methods to configure your IServiceCollection without including the background services for processing messages. AddFakeAsyncMonolithBaseServices configures fake services which don't depend on a database AddRealAsyncMonolithBaseServices configures real services without registering the background processors","title":"SetupTestHelpers"},{"location":"tests/#testconsumermessageprocessor","text":"Static methods for invoking and testing your consumers.","title":"TestConsumerMessageProcessor"},{"location":"tests/#consumertestbase","text":"The ConsumerTestBase offers a way to write simple tests for your consumers public class CancelShipmentTests : ConsumerTestBase { public CancelShipmentTests ( ITestOutputHelper testOutputHelper ) : base ( testOutputHelper ) { } private readonly string _inMemoryDatabaseName = Guid . NewGuid (). ToString (); protected override Task Setup ( IServiceCollection services ) { services . AddDbContext < ApplicationDbContext > (( sp , options ) => { options . UseInMemoryDatabase ( _inMemoryDatabaseName ); } ); services . AddRealAsyncMonolithBaseServices < ApplicationDbContext > ( settings => settings . RegisterTypesFromAssemblyContaining < Program > ()); return Task . CompletedTask ; } [Fact] public async Task OrderCancelled_Sets_Shipment_Status_To_Cancelled () { // Given var model = new Shipment { Id = \"test-shipment-id\" , OrderId = \"test-order-id\" , Status = \"pending\" , }; using ( var scope = Services . CreateScope ()) { var dbContext = scope . ServiceProvider . GetRequiredService < ApplicationDbContext > (); dbContext . Shipments . Add ( model ); await dbContext . SaveChangesAsync (); } // When await Process < CancelShipmentConsumer , OrderCancelled > ( new OrderCancelled () { OrderId = model . OrderId }); // Then using ( var scope = Services . CreateScope ()) { var dbContext = scope . ServiceProvider . GetRequiredService < ApplicationDbContext > (); var shipment = await dbContext . Shipments . FirstOrDefaultAsync ( c => c . Id == model . Id ); shipment . Status . Should (). Be ( \"cancelled\" ); await dbContext . SaveChangesAsync (); } } }","title":"ConsumerTestBase"},{"location":"warnings/","text":"Efcore does not natively support row level locking, this makes it possible for two instances of your app to compete over the next available message to be processed, potentially wasting cycles. For this reason it is reccomended that you only use AsyncMonolith.Ef when you are running a single instance of your app OR for development purposes. Using AsyncMonlith.PostgreSql / AsyncMonolith.MySql / AsyncMonolith.MsSql / AsyncMonolith.MariaDb will allow the system to lock rows ensuring they are only retrieved and processed once. Test your desired throughput All consumers must finish executing for a batch of messages before the next batch is started, therefore it is not currently reccomended to execute long running tasks inside your consumers. If you have a need for this please get in touch.","title":"Warnings \u26a0\ufe0f"},{"location":"guides/changing-messages/","text":"Backwards Compatibility : When modifying consumer payload schemas, ensure changes are backwards compatible so that existing messages with the old schema can still be processed. Schema Migration : If changes are not backwards compatible, make the changes in a copy of the ConsumerPayload (with a different class name) and update all consumers to operate on the new payload. Once all messages with the old payload schema have been processed, you can safely delete the old payload schema and its associated consumers.","title":"Changing messages"},{"location":"guides/consuming-messages/","text":"Independent Consumption : Each message will be consumed independently by each consumer set up to handle it. Periodic Querying : Each instance of your app will periodically query the consumer_messages table for a batch of available messages to process. The query takes place at the frequency defined by ProcessorMaxDelay , if a full batch is returned it will delay by ProcessorMinDelay . Concurrency : Each app instance can run multiple parallel consumer processors defined by ConsumerMessageProcessorCount , unless using AsyncMonolith.Ef . Batching : Consumer messages will be read from the consumer_messages table in batches defined by ConsumerMessageBatchSize . Idempotency : Ensure your Consumers are idempotent, since they will be retried on failure. Timeout : Consumers timeout after the number of seconds defined by the ConsumerTimeout attribute or the DefaultConsumerTimeout if not set. Example [ConsumerTimeout(5)] // Consumer timeouts after 5 seconds [ConsumerAttempts(1)] // Consumer messages moved to poisoned table after 1 failed attempt public class DeleteUsersPosts : BaseConsumer < UserDeleted > { private readonly ApplicationDbContext _dbContext ; public DeleteUsersPosts ( ApplicationDbContext dbContext ) { _dbContext = dbContext ; } public override Task Consume ( UserDeleted message , CancellationToken cancellationToken ) { ... await _dbContext . SaveChangesAsync ( cancellationToken ); } } Consumer Failures \ud83d\udca2 Retry Logic : Messages will be retried up to MaxAttempts times (with a AttemptDelay seconds between attempts) until they are moved to the poisoned_messages table. Add the [ConsumerAttempts(1)] attribute to override this behavior. Manual Intervention : If a message is moved to the poisoned_messages table, it will need to be manually removed from the database or moved back to the consumer_messages table to be retried. Note that the poisoned message will only be retried a single time unless you set attempts back to 0. Monitoring : Periodically monitor the poisoned_messages table to ensure there are not too many failed messages.","title":"Consuming messages"},{"location":"guides/consuming-messages/#consumer-failures","text":"Retry Logic : Messages will be retried up to MaxAttempts times (with a AttemptDelay seconds between attempts) until they are moved to the poisoned_messages table. Add the [ConsumerAttempts(1)] attribute to override this behavior. Manual Intervention : If a message is moved to the poisoned_messages table, it will need to be manually removed from the database or moved back to the consumer_messages table to be retried. Note that the poisoned message will only be retried a single time unless you set attempts back to 0. Monitoring : Periodically monitor the poisoned_messages table to ensure there are not too many failed messages.","title":"Consumer Failures \ud83d\udca2"},{"location":"guides/opentelemetry/","text":"Ensure you add AsyncMonolithInstrumentation.ActivitySourceName as a source to your OpenTelemetry configuration if you want to receive consumer / scheduled processor traces. builder . Services . AddOpenTelemetry () . WithTracing ( x => { if ( builder . Environment . IsDevelopment ()) x . SetSampler < AlwaysOnSampler > (); x . AddSource ( AsyncMonolithInstrumentation . ActivitySourceName ); x . AddConsoleExporter (); }) . ConfigureResource ( c => c . AddService ( \"async_monolith.demo\" ). Build ()); Tag Description consumer_message.id Consumer message Id consumer_message.attempt Attempt number consumer_message.payload.type Message payload type consumer_message.type Message consumer type exception.type Exception type exception.message Exception message","title":"Opentelemetry"},{"location":"guides/producing-messages/","text":"To produce messages in dotnet apps using AsyncMonolith you must first define a consumer payload class. This will act as the body of the message being passed to each consumer configured to handle it. public class OrderCancelled : IConsumerPayload { [JsonPropertyName(\"order_id\")] public string OrderId { get ; set ; } [JsonPropertyName(\"cancelled_at\")] public DateTimeOffset CancelledAt { get ; set ; } } When defining a consumer payload it must derive from the IConsumerPayload interface and be serializable by the System.Text.Json.JsonSerializer . As the consumer payload will be stored in the database in a serialized string, it is a good practice to keep it as small as possible. To produce your message you'll need to inject a IProducerService Immediate messages You can produce messages to be consumed immediately like this: order . Cancel (); _dbContext . Orders . Update ( order ); await _producerService . Produce ( new OrderCancelled () { OrderId = order . Id , CancelledAt = _timeProvider . UtcNow () }); // Save changes await _dbContext . SaveChangesAsync ( cancellationToken ); The message will be produced transactionally along with the change to your domain objects when you call SaveChangesAsync . Lean more about the Transactional Outbox pattern. If using the MySql, MsSql, MariaDb or PostgreSQL packages you will need to wrap your changes in a transaction see below Delayed messages You can produce messages to be consumed after a delay by specifying the number of seconds to wait before a consumer should process the message. order . Cancel (); _dbContext . Orders . Update ( order ); await _producerService . Produce ( new OrderCancelled () { OrderId = order . Id , CancelledAt = _timeProvider . UtcNow () }, 60 ); // Save changes await _dbContext . SaveChangesAsync ( cancellationToken ); Deduplicated messages Deduplicated messages are useful when you may emit the same message multiple times but only require it to be processed once for a given time period. For instance you may want to aggregate page views no more frequently then once every 10 seconds, you could schedule a reccuring message for this, but it may be wasteful if you anticipate pages go without views for extended periods of time. pageView . Increment (); _dbContext . PageViews . Update ( pageView ); await _producerService . Produce ( new PageViewed () { PageId = pageView . Id , }, 10 , $\"page_id:{pageView.Id}\" ); // Save changes await _dbContext . SaveChangesAsync ( cancellationToken ); Deduplicated events will ensure only a single message for a given consumer type and insertId are ever pending processing at any given time. MySql / MsSql / MariaDb / PostgreSql Transactionality The produce method makes use of ExecuteSqlRawAsync when using the MySql, MsSql or PostgreSQL package, if you want the messages to be inserted transactionally with your domain changes you must wrap all the changes in an explicit transaction. await using var dbContextTransaction = await _dbContext . Database . BeginTransactionAsync ( cancellationToken ); order . Cancel (); _dbContext . Orders . Update ( order ); await _producerService . Produce ( new OrderCancelled () { OrderId = order . Id , CancelledAt = _timeProvider . UtcNow () }); await _dbContext . SaveChangesAsync ( cancellationToken ); await dbContextTransaction . CommitAsync ( cancellationToken ); Summary Transactional Persistence : Produce messages along with changes to your DbContext before calling SaveChangesAsync , ensuring your domain changes and the messages they produce are persisted transactionally. Delay : Specify the number of seconds a message should remain in the queue before it is processed by a consumer. Deduplication : By specifying a insert_id when producing messages the system ensures only one message with the same insert_id and consumer_type will be in the table at a given time. This is useful when you need a process to take place an amount of time after the first action in a sequence occured.","title":"Producing messages"},{"location":"guides/producing-messages/#immediate-messages","text":"You can produce messages to be consumed immediately like this: order . Cancel (); _dbContext . Orders . Update ( order ); await _producerService . Produce ( new OrderCancelled () { OrderId = order . Id , CancelledAt = _timeProvider . UtcNow () }); // Save changes await _dbContext . SaveChangesAsync ( cancellationToken ); The message will be produced transactionally along with the change to your domain objects when you call SaveChangesAsync . Lean more about the Transactional Outbox pattern. If using the MySql, MsSql, MariaDb or PostgreSQL packages you will need to wrap your changes in a transaction see below","title":"Immediate messages"},{"location":"guides/producing-messages/#delayed-messages","text":"You can produce messages to be consumed after a delay by specifying the number of seconds to wait before a consumer should process the message. order . Cancel (); _dbContext . Orders . Update ( order ); await _producerService . Produce ( new OrderCancelled () { OrderId = order . Id , CancelledAt = _timeProvider . UtcNow () }, 60 ); // Save changes await _dbContext . SaveChangesAsync ( cancellationToken );","title":"Delayed messages"},{"location":"guides/producing-messages/#deduplicated-messages","text":"Deduplicated messages are useful when you may emit the same message multiple times but only require it to be processed once for a given time period. For instance you may want to aggregate page views no more frequently then once every 10 seconds, you could schedule a reccuring message for this, but it may be wasteful if you anticipate pages go without views for extended periods of time. pageView . Increment (); _dbContext . PageViews . Update ( pageView ); await _producerService . Produce ( new PageViewed () { PageId = pageView . Id , }, 10 , $\"page_id:{pageView.Id}\" ); // Save changes await _dbContext . SaveChangesAsync ( cancellationToken ); Deduplicated events will ensure only a single message for a given consumer type and insertId are ever pending processing at any given time.","title":"Deduplicated messages"},{"location":"guides/producing-messages/#mysql-mssql-mariadb-postgresql-transactionality","text":"The produce method makes use of ExecuteSqlRawAsync when using the MySql, MsSql or PostgreSQL package, if you want the messages to be inserted transactionally with your domain changes you must wrap all the changes in an explicit transaction. await using var dbContextTransaction = await _dbContext . Database . BeginTransactionAsync ( cancellationToken ); order . Cancel (); _dbContext . Orders . Update ( order ); await _producerService . Produce ( new OrderCancelled () { OrderId = order . Id , CancelledAt = _timeProvider . UtcNow () }); await _dbContext . SaveChangesAsync ( cancellationToken ); await dbContextTransaction . CommitAsync ( cancellationToken ); Summary Transactional Persistence : Produce messages along with changes to your DbContext before calling SaveChangesAsync , ensuring your domain changes and the messages they produce are persisted transactionally. Delay : Specify the number of seconds a message should remain in the queue before it is processed by a consumer. Deduplication : By specifying a insert_id when producing messages the system ensures only one message with the same insert_id and consumer_type will be in the table at a given time. This is useful when you need a process to take place an amount of time after the first action in a sequence occured.","title":"MySql / MsSql / MariaDb / PostgreSql Transactionality"},{"location":"guides/scheduling-messages/","text":"Frequency : Scheduled messages will be produced periodically by the chron_expression in the given chron_timezone Transactional Persistence : Schedule messages along with changes to your DbContext before calling SaveChangesAsync , ensuring your domain changes and the messages they produce are persisted transactionally. Processing : Schedule messages will be processed sequentially after they are made available by their chron job, at which point they will be turned into Consumer Messages and inserted into the consumer_messages table to be handled by their respective consumers. To produce your message you'll need to inject a IScheduleService // Publish 'CacheRefreshScheduled' every Monday at 12pm (UTC) with a tag that can be used to modify / delete related scheduled messages. _scheduleService . Schedule ( new CacheRefreshScheduled { Id = id }, \"0 0 12 * * MON\" , \"UTC\" , \"id:{id}\" ); await _dbContext . SaveChangesAsync ( cancellationToken );","title":"Scheduling messages"},{"location":"posts/idempotency/","text":"Idempotency refers to the ability of an operation to be performed multiple times whilst still yielding the same outcome. It is a critical property of handlers in an event driven system, since events can be reprocessed due to retries, network issues, or other failures. Without idempotency, reprocessing the same event could lead to inconsistent states, duplicate entries, or other unintended side effects. Let's consider a simple scenario where an event handler updates the status of an order. order . Status = \"cancelled\" ; await _dbContext . SaveChangesAsync (); await _emailService . SendCancellationEmail ( order . Id ); The issue with the above code is that if the event is processed twice the EmailService will send a duplicate email. We can achieve idempotency easily in this scenario by returning early if the order has already been cancelled. if ( order . Status == \"cancelled\" ){ return ; } order . Status = \"cancelled\" ; await _dbContext . SaveChangesAsync (); await _emailService . SendCancellationEmail ( order . Id ); You may notice an additional issue with the above example. If the EmailService throws an exception, the message will be retried but will exit early since the Status has already been set to cancelled . This problem can be solved by using AsyncMonolith to emit an OrderCancelled event transactionally along with the changes to your domain, which subsequently invokes a handler that sends an email. Check out the post on the Transactional Outbox pattern to learn more. The revised version using AsyncMonolith may look like this: if ( order . Status == \"cancelled\" ){ return ; } order . Status = \"cancelled\" ; await _producerService . Produce ( new OrderCancelled () { OrderId = order . Id }); await _dbContext . SaveChangesAsync (); public class SendOrderCancelledEmail : BaseConsumer < OrderCancelled > { public override Task Consume ( OrderCancelled message , CancellationToken cancellationToken ) { ... // Send order cancelled email } } By ensuring that your handlers are idempotent, you can safely reprocess events whilst making sure your system remains consistent and reliable.","title":"What is Idempotency?"},{"location":"posts/mediator/","text":"Let's consider a scenario where you are developing an ordering system. When a user cancels an order, you need to cancel the associated shipment and send the user an email confirming the cancellation. order . Cancel (); _dbContext . Orders . Update ( order ); await _dbContext . SaveChangesAsync (); _shipmentService . CancelShipment ( order . ShipmentId ); _emailService . SendCancellationEmail ( order . Id ); The problem with this approach is that each service is tightly coupled with its dependencies. The OrderService must know to tell ShipmentService and the EmailService that an order has been cancelled. In a simple system, this might not cause significant problems, but as it grows, the number of connections between classes can make them difficult to maintain. The Mediator pattern aims to solve this problem by introducing a Mediator service which acts as a coordinator between services. Each service sends requests to the Mediator without needing to concern itself with the responsibilities of other services. After refactoring the OrderService , it now simply updates the order domain object and tells the Mediator that an order has been canceled: order . Cancel (); _dbContext . Orders . Update ( order ); await _dbContext . SaveChangesAsync (); _mediator . Send ( new OrderCancelled ( order . Id )); It's the Mediator's job to route the request to each handler configured to handle the OrderCancelled event within their own context. For example the handlers in this scenario could be implemented like this: public class CancelShipmentHandler { public async Task Handle ( OrderCancelled orderCancelled ) { ... shipment . Cancel (); _dbContext . Shipments . Update ( shipment ); await _dbContext . SaveChangesAsync (); } } public class CancellationEmailHandler { public async Task Handle ( OrderCancelled orderCancelled ) { ... // Send order cancelled email } } This pattern promotes: Code reuse You may have multiple places where an order can be canceled. Now, you don't have to duplicate the logic to cancel the shipment, send an email, or coordinate those actions when an order is cancelled. Single responsibility principle Each handler is responsible for handling the OrderCancelled event within its own context. Open/closed principle Additional handlers can be easily added to extend the behavior of your system without modifying existing code. Testability Handlers can be unit tested in isolation. One issue with the mediator pattern is that a part of your system could fail without recourse. For instance, if the CancelShipmentHandler fails, your system could be left in an inconsistent state since the order has been canceled, the email has been sent but shipment will still be made. AsyncMonolith acts as a mediator, providing the benefits of decoupling while ensuring transactional consistency by using the Transactional Outbox pattern. This ensures that each message is stored in your database before being handled, so if anything fails, it will be retried multiple times before being moved into a poisoned_messages table where you can manually intervene. Refactoring the above scenario to use AsyncMonolith may look like this: order . Cancel (); _dbContext . Orders . Update ( order ); await _producerService . Produce ( new OrderCancelled () { OrderId = order . Id }); await _dbContext . SaveChangesAsync (); public class CancelShipment : BaseConsumer < OrderCancelled > { public override Task Consume ( OrderCancelled message , CancellationToken cancellationToken ) { ... shipment . Cancel (); _dbContext . Shipments . Update ( shipment ); await _dbContext . SaveChangesAsync (); } } public class SendOrderCancelledEmail : BaseConsumer < OrderCancelled > { public override Task Consume ( OrderCancelled message , CancellationToken cancellationToken ) { ... // Send order cancelled email } }","title":"What is the Mediator pattern?"},{"location":"posts/mediator/#code-reuse","text":"You may have multiple places where an order can be canceled. Now, you don't have to duplicate the logic to cancel the shipment, send an email, or coordinate those actions when an order is cancelled.","title":"Code reuse"},{"location":"posts/mediator/#single-responsibility-principle","text":"Each handler is responsible for handling the OrderCancelled event within its own context.","title":"Single responsibility principle"},{"location":"posts/mediator/#openclosed-principle","text":"Additional handlers can be easily added to extend the behavior of your system without modifying existing code.","title":"Open/closed principle"},{"location":"posts/mediator/#testability","text":"Handlers can be unit tested in isolation. One issue with the mediator pattern is that a part of your system could fail without recourse. For instance, if the CancelShipmentHandler fails, your system could be left in an inconsistent state since the order has been canceled, the email has been sent but shipment will still be made. AsyncMonolith acts as a mediator, providing the benefits of decoupling while ensuring transactional consistency by using the Transactional Outbox pattern. This ensures that each message is stored in your database before being handled, so if anything fails, it will be retried multiple times before being moved into a poisoned_messages table where you can manually intervene. Refactoring the above scenario to use AsyncMonolith may look like this: order . Cancel (); _dbContext . Orders . Update ( order ); await _producerService . Produce ( new OrderCancelled () { OrderId = order . Id }); await _dbContext . SaveChangesAsync (); public class CancelShipment : BaseConsumer < OrderCancelled > { public override Task Consume ( OrderCancelled message , CancellationToken cancellationToken ) { ... shipment . Cancel (); _dbContext . Shipments . Update ( shipment ); await _dbContext . SaveChangesAsync (); } } public class SendOrderCancelledEmail : BaseConsumer < OrderCancelled > { public override Task Consume ( OrderCancelled message , CancellationToken cancellationToken ) { ... // Send order cancelled email } }","title":"Testability"},{"location":"posts/transactional-outbox/","text":"Consider a scenario where when a user places an order, you also need to create an entity that tracks the shipment. This scenario is fine as the order and shipment are both committed to your database transactionally; either both succeed, or neither do. You will never have an order created without a shipment. _dbContext . Orders . Add ( newOrder ); _dbContext . Shipments . Add ( newShipment ); await _dbContext . SaveChangesAsync (); As your application grows, you find that an increasing number of things need to happen when an order is created, and some of them have their own chain of operations. This will quickly cause any method that needs to create an order to grow in complexity and scope. As a result you decide you want to decouple the process of creating an order from the process of creating a shipment, so you adopt an event-driven approach. However, now you have to make a difficult decision: should I commit my order to my database first, then produce an event, or should I produce the event first, then commit the order to my database? In the first scenario, it is possible an order is created but publishing the 'OrderCreated' event fails. // Create order _dbContext . Orders . Add ( newOrder ); await _dbContext . SaveChangesAsync (); // Dispatch event channel . BasicPublish (...); \\\\ This fails In the second scenario, it is possible that an event is published, but the order fails to be created. // Dispatch event channel . BasicPublish (...); // Create order _dbContext . Orders . Add ( newOrder ); await _dbContext . SaveChangesAsync (); \\\\ This fails The transactional outbox is a pattern which solves this problem. The general idea is that instead of publishing an event along with making a change to your database, you create an entity that will publish an event at a later time and commit both changes to the database as part of the same transaction. You then have a process capable of checking for records in the outbox table and publishing them as events while handling failures and retries. // Create order _dbContext . Orders . Add ( newOrder ); // Insert event into outbox _dbContext . Outbox . Add ( orderCreated ) await _dbContext . SaveChangesAsync (); For simple use cases, you may realize that the outbox message can act as the event, and you don't actually need the additional step of publishing to a message broker. Instead, you write your events to your database along with the changes to your domain and have a process that periodically fetches, processes, and then removes messages from the outbox table. This is one of the core principles behind AsyncMonolith . In summary, the transactional outbox pattern ensures that events are reliably published, maintaining consistency between operations. While it introduces the need for an additional process to handle the outbox and increases the load on your application database, the benefits of decoupling and reliability often outweigh these overheads. Here is how the above example may look if you\u2019re using AsyncMonolith // Create order _dbContext . Orders . Add ( newOrder ); // Insert event into outbox await _producerService . Produce ( new OrderCreated () { OrderId = order . Id }); await _dbContext . SaveChangesAsync (); public class CreateShipment : BaseConsumer < OrderCreated > { public override Task Consume ( OrderCreated message , CancellationToken cancellationToken ) { ... // Create Order } }","title":"What is the Transactional Outbox?"}]}